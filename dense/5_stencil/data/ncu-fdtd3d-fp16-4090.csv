1160,==PROF== Connected to process 818106 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1164 x 1164 x 1164 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 37x73 GPU FDTD loopiter = 1dimx = 1164gpu_time = 23372.930050fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 818106==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10208230741.43,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2227606251.22,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,45107098,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.46,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.95,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,20248512,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.08,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.52,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,43614232.48,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.46,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.92,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.56,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,342520148048.41,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.15,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.46,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.92,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.15,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.46,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.92,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.08,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.68,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.68,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.7% of the total average of 23.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,7378456.75,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3777769856,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,7378590.33,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3777838249,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Block Size,,512,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Grid Size,,2701,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Threads,thread,1382912,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Waves Per SM,,21.1,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",818106,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1152,==PROF== Connected to process 822251 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1156 x 1156 x 1156 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 37x73 GPU FDTD loopiter = 1dimx = 1156gpu_time = 23332.752943fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 822251==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10283057540.73,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2243937994.99,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,44913845,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.34,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.83,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,20015040,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.8,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.3,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,43569495.21,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.34,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.82,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.46,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,343812166850.53,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.09,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.34,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.83,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.21,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.34,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.85,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.15,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.98,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.62,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.62,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.8% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,7328741.47,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3752315632,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,7328873.7,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3752383336,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Block Size,,512,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Grid Size,,2701,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Threads,thread,1382912,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Launch Statistics,Waves Per SM,,21.1,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(37, 73, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",822251,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1144,==PROF== Connected to process 826306 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1148 x 1148 x 1148 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 36x72 GPU FDTD loopiter = 1dimx = 1148gpu_time = 22234.952927fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 826306==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10243517602.18,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235312823.2,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,42573255,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.6,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.5,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,19045216,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.14,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.6,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,41232954.55,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.6,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.94,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.58,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,349110573069.9,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.22,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.6,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.05,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.31,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.6,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.94,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.06,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.67,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.67,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6985278,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3576462336,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6985407.84,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3576528812,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Block Size,,512,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Grid Size,,2592,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Threads,thread,1327104,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Waves Per SM,,20.25,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",826306,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1136,==PROF== Connected to process 830207 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1140 x 1140 x 1140 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 36x72 GPU FDTD loopiter = 1dimx = 1140gpu_time = 22093.869925fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 830207==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10281155444.52,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2243532440.22,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,42187988,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.7,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.34,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,18803776,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.24,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.55,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,40867934.52,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.7,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.98,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.62,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,348758390655.15,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.27,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.7,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.82,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.6,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.7,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.01,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.99,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.02,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.02 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.62,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.62,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.7% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6937569,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3552035328,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6937698.21,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3552101486,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Block Size,,512,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Grid Size,,2592,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Threads,thread,1327104,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Launch Statistics,Waves Per SM,,20.25,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 72, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",830207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1128,==PROF== Connected to process 834048 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1132 x 1132 x 1132 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 36x71 GPU FDTD loopiter = 1dimx = 1132gpu_time = 21603.365898fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 834048==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10299181789.56,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2247471175.36,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,41272044,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.75,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.01,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,18363328,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.24,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.85,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,40019452.33,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.75,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.98,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.62,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,346194251499.51,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.29,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.75,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.21,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.04,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.75,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.97,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.03,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.6,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.6,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6794167.5,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3478613760,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6794295.98,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3478679541,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Block Size,,512,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Grid Size,,2556,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Threads,thread,1308672,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Waves Per SM,,19.97,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",834048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1120,==PROF== Connected to process 837814 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1124 x 1124 x 1124 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 36x71 GPU FDTD loopiter = 1dimx = 1124gpu_time = 21551.801920fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 837814==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10208697954,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2227728083.15,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,41473751,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.18,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.89,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,18616608,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,47.3,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.29,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,41371958.49,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.18,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.65,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.31,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.65,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,16.92,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,332144572845.92,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.18,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.62,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.35,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.18,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.27,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.16,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.73,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.34,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,24.58,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,24.58,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 24.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6747121.12,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3454526016,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6747250.08,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3454592039,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Block Size,,512,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Grid Size,,2556,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Threads,thread,1308672,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Launch Statistics,Waves Per SM,,19.97,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(36, 71, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",837814,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1112,==PROF== Connected to process 841526 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1116 x 1116 x 1116 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 35x70 GPU FDTD loopiter = 1dimx = 1116gpu_time = 20595.823050fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 841526==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10225729979.76,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2231440056.12,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,39520023,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.1,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.43,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,17710080,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.61,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.46,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,38295639.24,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.1,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.74,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.39,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,347817476149.18,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.97,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.1,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.95,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,69.98,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.1,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.73,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.27,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.86,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.86,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.8% of the total average of 23.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6412415.62,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3283156800,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6412540.19,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3283220578,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Block Size,,512,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Grid Size,,2450,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Threads,thread,1254400,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Waves Per SM,,19.14,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",841526,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1104,==PROF== Connected to process 845158 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1108 x 1108 x 1108 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 35x70 GPU FDTD loopiter = 1dimx = 1108gpu_time = 20413.322926fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 845158==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10216311320.34,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2229379532.6,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,39507803,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.78,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.09,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,17720928,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.58,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.06,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,38040298.58,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.78,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.75,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.39,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,334321294234.7,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.8,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.78,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.19,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.78,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.72,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.28,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.9,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.91,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6370153.12,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3261518400,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6370278.36,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3261582518,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Block Size,,512,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Grid Size,,2450,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Threads,thread,1254400,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Launch Statistics,Waves Per SM,,19.14,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 70, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",845158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1096,==PROF== Connected to process 848710 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1100 x 1100 x 1100 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 35x69 GPU FDTD loopiter = 1dimx = 1100gpu_time = 19925.907135fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 848710==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10142537351.11,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2213287082.33,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,37872755,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.76,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.19,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,17111136,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.95,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.67,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,36952520.39,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.76,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.87,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.51,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,342658535353.82,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.3,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.76,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.88,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.57,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.76,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.83,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.17,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.81,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.81,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6234699.84,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3192166320,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6234824,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3192229887,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Block Size,,512,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Grid Size,,2415,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Threads,thread,1236480,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Waves Per SM,,18.87,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",848710,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1088,==PROF== Connected to process 852173 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1092 x 1092 x 1092 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 35x69 GPU FDTD loopiter = 1dimx = 1092gpu_time = 20002.399921fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 852173==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10188956499.18,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2223411407.8,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,36844667,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.74,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.14,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,16570784,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.1,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.92,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,36568531.41,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.74,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.93,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.57,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,343681838107.36,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.8,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.74,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.03,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.92,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.74,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.91,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.09,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.61,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.61,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6190248.75,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3169407360,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6190373.38,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3169471172,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Block Size,,512,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Grid Size,,2415,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Threads,thread,1236480,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Launch Statistics,Waves Per SM,,18.87,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(35, 69, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",852173,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1080,==PROF== Connected to process 855584 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1084 x 1084 x 1084 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 34x68 GPU FDTD loopiter = 1dimx = 1084gpu_time = 19179.120064fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 855584==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10231554288.39,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232713200.71,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,36436866,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.84,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.99,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,16319168,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.82,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.34,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,34959409.89,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.84,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.83,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.47,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,343653135257.88,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.83,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.84,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.65,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.61,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.84,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.76,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.24,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.98,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.77,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.77,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.8% of the total average of 23.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5883678.75,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3012443520,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5883799.6,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3012505393,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Block Size,,512,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Grid Size,,2312,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Threads,thread,1183744,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Waves Per SM,,18.06,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",855584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1072,==PROF== Connected to process 858906 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1076 x 1076 x 1076 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 34x68 GPU FDTD loopiter = 1dimx = 1076gpu_time = 19001.463890fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 858906==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10238871735.75,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234313392.05,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,36218010,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.78,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.14,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,16209536,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.68,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.05,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,34803725.59,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.78,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.78,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.42,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,335603646643.56,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.8,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.78,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.97,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,70.85,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.78,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.82,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.18,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.76,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.76,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5841123.5,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2990655232,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5841246.01,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2990717959,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Block Size,,512,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Grid Size,,2312,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Threads,thread,1183744,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Launch Statistics,Waves Per SM,,18.06,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 68, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",858906,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1064,==PROF== Connected to process 862171 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1068 x 1068 x 1068 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 34x67 GPU FDTD loopiter = 1dimx = 1068gpu_time = 18059.187889fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 862171==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10303316893.71,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2248369299.56,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,34356302,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.23,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.16,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,15280160,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.09,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.95,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,33749136.78,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.23,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.93,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.57,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,347794163411.9,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.54,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.23,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.66,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.11,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.23,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.88,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.12,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.63,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.63,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5713295.19,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2925207136,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5713414.43,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2925268190,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Block Size,,512,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Grid Size,,2278,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Threads,thread,1166336,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Waves Per SM,,17.8,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",862171,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1056,==PROF== Connected to process 865352 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1060 x 1060 x 1060 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 34x67 GPU FDTD loopiter = 1dimx = 1060gpu_time = 18138.299942fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 865352==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10304234312.12,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2248564089.61,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,33706703,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.79,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.12,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,14989920,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.51,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.13,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,33217396.07,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.79,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.07,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.72,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,347428411625.95,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.83,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.79,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.45,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.26,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.79,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.13,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.87,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.42,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.42,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5671365.75,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2903739264,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5671486.06,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2903800863,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Block Size,,512,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Grid Size,,2278,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Threads,thread,1166336,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Launch Statistics,Waves Per SM,,17.8,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(34, 67, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",865352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1048,==PROF== Connected to process 868482 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1052 x 1052 x 1052 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 33x66 GPU FDTD loopiter = 1dimx = 1052gpu_time = 17633.590221fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 868482==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10251366261.44,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237029144.91,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,33241008,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.95,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.68,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,14859040,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.03,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.52,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,31832959.36,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.95,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.91,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.54,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,341293856938.27,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.89,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.95,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.48,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.09,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.95,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.88,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.12,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.65,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.65,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5382314.44,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2755744992,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5382431.23,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2755804788,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Block Size,,512,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Grid Size,,2178,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Threads,thread,1115136,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Waves Per SM,,17.02,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",868482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1040,==PROF== Connected to process 871533 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1044 x 1044 x 1044 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 33x66 GPU FDTD loopiter = 1dimx = 1044gpu_time = 17748.546839fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 871533==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10270849917.44,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2241281633.9,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,32706352,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.55,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,14592288,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.66,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.59,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,31172701.29,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.11,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.77,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,340686512766.2,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.08,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.32,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.28,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.09,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.91,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.43,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.43,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.6% of the total average of 23.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5333513.62,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2730758976,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5333630.56,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2730818848,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Block Size,,512,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Grid Size,,2178,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Threads,thread,1115136,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Launch Statistics,Waves Per SM,,17.02,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 66, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",871533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1032,==PROF== Connected to process 874533 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1036 x 1036 x 1036 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 33x65 GPU FDTD loopiter = 1dimx = 1036gpu_time = 16854.755878fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 874533==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10231706663.12,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232727968.46,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,30645693,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.37,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.88,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,13725216,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.84,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.35,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,30357899.15,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.37,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.18,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.83,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,352470034715.66,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.12,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.37,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.26,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.1,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.37,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.21,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.79,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.25,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.25,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5215701.56,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2670439200,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5215817.16,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2670498387,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Block Size,,512,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Grid Size,,2145,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Threads,thread,1098240,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Waves Per SM,,16.76,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",874533,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1024,==PROF== Connected to process 877480 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1028 x 1028 x 1028 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 33x65 GPU FDTD loopiter = 1dimx = 1028gpu_time = 17632.905006fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 877480==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10230994111.66,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232581050.71,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,31036804,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.38,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.51,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,13901376,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.02,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.73,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,30627577.22,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.38,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.9,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.54,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,338977464101.4,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.61,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.38,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.02,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.44,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.38,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.93,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.07,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.65,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.65,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,5176220.16,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2650224720,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,5176335.54,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2650283795,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Block Size,,512,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Grid Size,,2145,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Threads,thread,1098240,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Launch Statistics,Waves Per SM,,16.76,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(33, 65, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",877480,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1016,==PROF== Connected to process 880376 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1020 x 1020 x 1020 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 32x64 GPU FDTD loopiter = 1dimx = 1020gpu_time = 15983.300924fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 880376==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10234606219.35,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233382134.97,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,28622305,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.7,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,36.12,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12815392,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.24,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.98,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,28315606.01,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.7,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.69,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.32,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.98,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,354887767459.63,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.29,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.7,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,8.94,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.86,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.7,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.3,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.7,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.13,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.14,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4904448,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2511077376,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4904561.09,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2511135280,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Block Size,,512,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Grid Size,,2048,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Threads,thread,1048576,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Waves Per SM,,16,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",880376,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1008,==PROF== Connected to process 883186 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1012 x 1012 x 1012 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 32x64 GPU FDTD loopiter = 1dimx = 1012gpu_time = 16190.551996fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 883186==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10320090460.01,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2251984646.57,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,28063746,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.29,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,36.11,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12461344,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.86,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,25.15,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,27751638.19,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.29,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.69,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.54,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.2,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,357711276087.07,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.59,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.29,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,8.74,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.98,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.29,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.54,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.46,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.84,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.84,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4866752,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2491777024,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4866864.22,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2491834482,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Block Size,,512,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Grid Size,,2048,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Threads,thread,1048576,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Launch Statistics,Waves Per SM,,16,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 64, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",883186,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
1000,==PROF== Connected to process 885946 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 1004 x 1004 x 1004 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 32x63 GPU FDTD loopiter = 1dimx = 1004gpu_time = 15754.528046fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 885946==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10244152958.54,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235457487.93,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,27988784,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.25,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.23,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12520064,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.32,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,27568812.79,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.25,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.24,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.89,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,346503675380.57,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.06,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.25,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.07,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.72,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.25,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.24,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.76,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.19,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.2,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4753602,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2433844224,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4753713.2,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2433901159,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Block Size,,512,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Grid Size,,2016,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Threads,thread,1032192,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Waves Per SM,,15.75,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",885946,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
992,==PROF== Connected to process 888644 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 996 x 996 x 996 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 32x63 GPU FDTD loopiter = 1dimx = 996gpu_time = 15641.790867fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 888644==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10222123145.99,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230647080.95,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,27538467,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.66,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.08,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12345152,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.35,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.47,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,27162463.86,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.66,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.69,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.36,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.02,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,344281212900.42,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.27,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.66,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.68,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.9,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.66,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.36,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.64,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.02,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.02,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4716495,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2414845440,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4716605.85,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2414902195,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Block Size,,512,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Grid Size,,2016,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Threads,thread,1032192,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Launch Statistics,Waves Per SM,,15.75,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(32, 63, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",888644,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
984,==PROF== Connected to process 891283 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 988 x 988 x 988 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 31x62 GPU FDTD loopiter = 1dimx = 988gpu_time = 14947.609186fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 891283==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10244883195.02,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235621875.59,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,27491537,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.05,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.33,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12296736,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.68,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.5,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,26036679.07,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.05,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.13,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.78,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,337625854535.71,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.94,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.05,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.7,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.47,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.05,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.07,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.93,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.36,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.36,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4461202.25,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2284135552,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4461312.45,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2284191974,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Block Size,,512,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Grid Size,,1922,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Threads,thread,984064,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Waves Per SM,,15.02,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",891283,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
976,==PROF== Connected to process 893856 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 980 x 980 x 980 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 31x62 GPU FDTD loopiter = 1dimx = 980gpu_time = 15151.402950fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 893856==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10263756363.42,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2239722157.18,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,27116510,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.32,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.19,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,12106688,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.84,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.33,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,25744621.9,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.32,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.19,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.83,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,336833644346,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.08,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.32,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.08,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.31,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.32,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.2,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.8,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.28,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.28,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4425825.44,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2266022624,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4425934.76,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2266078599,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Block Size,,512,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Grid Size,,1922,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Threads,thread,984064,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Launch Statistics,Waves Per SM,,15.02,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 62, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",893856,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
968,==PROF== Connected to process 896360 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 972 x 972 x 972 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 31x61 GPU FDTD loopiter = 1dimx = 972gpu_time = 14484.011889fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 896360==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10232228003.85,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232844195.34,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,25609924,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.09,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,11469280,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.53,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.31,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,25266673.36,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.07,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.72,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,344669547870.49,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.55,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.73,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.06,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.94,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.42,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.42,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4312070.94,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2207780320,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4312178.34,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2207835309,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Block Size,,512,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Grid Size,,1891,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Threads,thread,968192,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Waves Per SM,,14.77,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",896360,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
960,==PROF== Connected to process 898794 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 964 x 964 x 964 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 31x61 GPU FDTD loopiter = 1dimx = 964gpu_time = 14261.117935fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 898794==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10274815778.07,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2242120038.61,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,25105203,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.44,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.09,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,11196640,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.32,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.34,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,24664732.11,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.44,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.35,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.01,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,346086328755.77,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.16,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.44,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.68,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.69,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.44,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.34,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.66,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.07,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.07,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4279451.19,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2191079008,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4279558.99,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2191134201,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Block Size,,512,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Grid Size,,1891,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Threads,thread,968192,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Launch Statistics,Waves Per SM,,14.77,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(31, 61, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",898794,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
952,==PROF== Connected to process 902043 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 956 x 956 x 956 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 30x60 GPU FDTD loopiter = 1dimx = 956gpu_time = 14074.915886fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 902043==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10239074359.84,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234334752.75,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,25113851,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.66,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.99,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,11239616,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.18,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.3,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,23823133.36,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.66,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.96,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.6,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,334079216941.22,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.74,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.66,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.48,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.63,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.66,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.95,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.05,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.57,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.57,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.5% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4040381.25,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2068675200,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4040485.92,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2068728791,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Block Size,,512,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Grid Size,,1800,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Threads,thread,921600,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Waves Per SM,,14.06,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",902043,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
944,==PROF== Connected to process 904792 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 948 x 948 x 948 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 30x60 GPU FDTD loopiter = 1dimx = 948gpu_time = 13678.699970fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 904792==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10235744769.4,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233627458.5,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,24960822,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.55,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.44,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,11174752,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.08,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.04,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,23676037.87,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.55,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.93,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.56,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,328620072105.4,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.69,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.55,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.53,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.68,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.55,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.89,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.11,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.64,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.64,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,4007250,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2051712000,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,4007355.34,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2051765932,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Block Size,,512,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Grid Size,,1800,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Threads,thread,921600,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Launch Statistics,Waves Per SM,,14.06,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 60, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",904792,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
936,==PROF== Connected to process 907075 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 940 x 940 x 940 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 30x59 GPU FDTD loopiter = 1dimx = 940gpu_time = 13256.478071fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 907075==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10218652769.38,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2229874265.55,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,23003439,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.28,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,10315680,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.78,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.36,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,22761920.12,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.17,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.81,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,346126608037.47,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.06,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.96,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.65,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.18,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.82,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.26,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3907883.44,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,2000836320,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3907987.9,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,2000889803,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Block Size,,512,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Grid Size,,1770,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Threads,thread,906240,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Waves Per SM,,13.83,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",907075,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
928,==PROF== Connected to process 909307 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 932 x 932 x 932 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 30x59 GPU FDTD loopiter = 1dimx = 932gpu_time = 13079.220057fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 909307==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10261295709.58,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2239192148.37,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,22956101,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.94,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.64,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,10251680,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.75,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.17,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,22582884.39,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.94,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.16,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.8,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,341204426201.36,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.9,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.94,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.46,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.82,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.94,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.15,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.85,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.29,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.29,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3875304.38,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1984155840,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3875408.02,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1984208906,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Block Size,,512,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Grid Size,,1770,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Threads,thread,906240,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Launch Statistics,Waves Per SM,,13.83,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(30, 59, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",909307,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
920,==PROF== Connected to process 911496 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 924 x 924 x 924 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 29x58 GPU FDTD loopiter = 1dimx = 924gpu_time = 12499.306917fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 911496==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10235973739.38,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233657856.7,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,22424998,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.21,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.32,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,10039264,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.53,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.49,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,21373847.69,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.21,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.09,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.72,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,337216879643.77,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.02,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.21,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.05,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.39,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.21,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.1,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.9,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.41,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.41,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3651674.56,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1869657376,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3651776.97,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1869709810,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Block Size,,512,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Grid Size,,1682,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Threads,thread,861184,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Waves Per SM,,13.14,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",911496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
912,==PROF== Connected to process 913601 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 916 x 916 x 916 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 29x58 GPU FDTD loopiter = 1dimx = 916gpu_time = 12165.822983fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 913601==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10231553449.58,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232708957.28,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,22889126,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.85,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.89,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,10251456,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.35,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.74,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,21705998.54,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.85,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.68,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.3,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,323043345257.49,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.33,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.85,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.61,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.71,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.85,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.7,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.3,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,24,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,24,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 24.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3620715.25,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1853806208,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3620816.74,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1853858169,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Block Size,,512,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Grid Size,,1682,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Threads,thread,861184,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Launch Statistics,Waves Per SM,,13.14,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 58, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",913601,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
904,==PROF== Connected to process 915715 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 908 x 908 x 908 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 29x57 GPU FDTD loopiter = 1dimx = 908gpu_time = 12018.034935fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 915715==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10222494493.39,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230718405.05,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,20741680,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.3,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.14,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,9297920,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.77,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.45,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,20545548.43,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.3,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.17,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.81,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,344874229074.89,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.08,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.3,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.78,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.87,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.3,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.16,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.84,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.31,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.31,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3527863.59,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1806266160,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3527963.68,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1806317402,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Block Size,,512,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Grid Size,,1653,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Threads,thread,846336,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.91,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",915715,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
896,==PROF== Connected to process 917736 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 900 x 900 x 900 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 29x57 GPU FDTD loopiter = 1dimx = 900gpu_time = 11999.274015fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 917736==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10236101999.47,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233679029.39,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,20592965,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.19,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.63,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,9218976,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.67,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.13,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,20392346.9,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.19,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.68,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.12,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.68,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.77,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,340337460472.83,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.03,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.19,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.9,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.87,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.19,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.16,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.84,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.32,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.32,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3490826.06,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1787302944,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3490927.72,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1787354992,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Block Size,,512,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Grid Size,,1653,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Threads,thread,846336,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.91,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(29, 57, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",917736,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
888,==PROF== Connected to process 919712 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 892 x 892 x 892 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 28x56 GPU FDTD loopiter = 1dimx = 892gpu_time = 11862.106085fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 919712==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10232680863.27,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232917544.58,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,19931988,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.78,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.66,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,8926080,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.23,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.56,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,18959616.25,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.78,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.32,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.97,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,340459611385.96,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.31,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.78,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.47,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.43,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.78,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.31,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.69,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.07,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.07,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3284274,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1681548288,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3284371.76,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1681598342,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Block Size,,512,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Grid Size,,1568,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Threads,thread,802816,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.25,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",919712,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
880,==PROF== Connected to process 921648 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 884 x 884 x 884 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 28x56 GPU FDTD loopiter = 1dimx = 884gpu_time = 11745.323896fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 921648==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10236929418.2,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233860531.06,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,19829118,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.6,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.11,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,8876320,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.95,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.44,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,18895647.55,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.6,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.23,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.87,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,335212819727.09,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.22,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.6,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.86,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.66,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.6,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.23,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.77,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.22,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.22,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3255413,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1666771456,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3255509.98,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1666821111,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Block Size,,512,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Grid Size,,1568,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Threads,thread,802816,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.25,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 56, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",921648,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
872,==PROF== Connected to process 923546 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 876 x 876 x 876 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 28x55 GPU FDTD loopiter = 1dimx = 876gpu_time = 11815.521002fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 923546==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10245273597.05,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235673707.12,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,19437361,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.27,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.66,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,8693856,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.32,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.44,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,18258073.55,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.27,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.36,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.01,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,331020593853.87,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.05,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.27,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.71,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.95,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.27,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.38,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.62,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.05,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.05,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3168935,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1622494720,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3169031.62,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1622544191,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Block Size,,512,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Grid Size,,1540,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Threads,thread,788480,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.03,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",923546,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
864,==PROF== Connected to process 925407 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 868 x 868 x 868 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 28x55 GPU FDTD loopiter = 1dimx = 868gpu_time = 11197.230816fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 925407==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10234549090.33,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233355457.52,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,19418953,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.88,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.88,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,8694720,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.85,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.98,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,18260427.61,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.88,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.2,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.84,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,323044451805.23,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.85,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.88,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.99,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.96,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.88,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.21,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.79,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.2,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.2,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,3140589.38,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1607981760,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,3140686.22,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1608031345,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Block Size,,512,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Grid Size,,1540,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Threads,thread,788480,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Launch Statistics,Waves Per SM,,12.03,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(28, 55, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",925407,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
856,==PROF== Connected to process 927229 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 860 x 860 x 860 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 27x54 GPU FDTD loopiter = 1dimx = 860gpu_time = 10586.487055fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 927229==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10234325380.99,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233294704.25,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,17914099,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.67,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.47,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,8021120,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.91,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.73,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,17111005.01,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.67,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.22,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.86,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,338697390888.06,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.26,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.67,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.03,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.68,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.67,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.22,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.78,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.2,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.2,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.4% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2946526.88,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1508621760,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2946620.91,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1508669908,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Block Size,,512,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Grid Size,,1458,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Threads,thread,746496,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Waves Per SM,,11.39,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",927229,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
848,==PROF== Connected to process 928972 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 852 x 852 x 852 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 27x54 GPU FDTD loopiter = 1dimx = 852gpu_time = 10272.216082fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 928972==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10253965316.8,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237569427.1,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,17800471,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.54,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.93,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,7954976,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.77,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.42,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,17001540.45,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.54,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.17,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.81,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,333991544411.95,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.19,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.54,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.08,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.69,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.54,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.16,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.84,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.32,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.32,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2919690.56,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1494881568,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2919784.61,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1494929719,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Block Size,,512,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Grid Size,,1458,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Threads,thread,746496,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Launch Statistics,Waves Per SM,,11.39,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 54, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",928972,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
840,==PROF== Connected to process 930657 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 844 x 844 x 844 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 27x53 GPU FDTD loopiter = 1dimx = 844gpu_time = 10147.712946fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 930657==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10248743377.82,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236428260.91,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,17413314,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.25,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.56,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,7785952,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.29,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.53,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,16361162.02,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.25,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.35,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.99,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,330208728746.34,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.04,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.25,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.43,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.09,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.25,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.34,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.66,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.07,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.07,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2839282.88,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1453712832,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2839376.48,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1453760759,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Block Size,,512,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Grid Size,,1431,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Threads,thread,732672,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Waves Per SM,,11.18,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",930657,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
832,==PROF== Connected to process 932322 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 836 x 836 x 836 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 27x53 GPU FDTD loopiter = 1dimx = 836gpu_time = 10079.141855fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 932322==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10242310499.43,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235017610.52,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,17378135,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.9,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.84,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,7775104,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.82,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.97,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,16360566.46,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.9,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.19,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.83,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,322945952620.05,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.86,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.9,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.95,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.99,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.9,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.18,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.82,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.25,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.25,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2812943.53,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1440227088,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2813037.4,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1440275151,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Block Size,,512,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Grid Size,,1431,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Threads,thread,732672,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Launch Statistics,Waves Per SM,,11.18,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(27, 53, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",932322,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
824,==PROF== Connected to process 933944 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 828 x 828 x 828 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 26x52 GPU FDTD loopiter = 1dimx = 828gpu_time = 9482.359886fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 933944==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10211189576.48,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2228234006.18,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,15681160,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.61,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.01,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,7037216,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.23,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.97,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,15175311.38,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.61,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.31,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.97,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,343223438359.71,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.73,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.61,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.5,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.63,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.61,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.31,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.69,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.1,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.1,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2627358.5,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1345207552,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2627449.82,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1345254309,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Block Size,,512,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Grid Size,,1352,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Threads,thread,692224,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Waves Per SM,,10.56,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",933944,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
816,==PROF== Connected to process 935496 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 820 x 820 x 820 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 26x52 GPU FDTD loopiter = 1dimx = 820gpu_time = 9481.378078fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 935496==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10241312711.67,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234792465.18,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,15527456,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.62,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.55,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6947776,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.41,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.9,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,14974709.88,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.62,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.39,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.04,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,339683213736.31,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.74,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.62,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.98,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.83,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.62,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.38,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.62,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.99,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.99,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.3% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2604036.5,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1333266688,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2604126.99,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1333313021,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Block Size,,512,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Grid Size,,1352,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Threads,thread,692224,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Launch Statistics,Waves Per SM,,10.56,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 52, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",935496,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
808,==PROF== Connected to process 937018 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 812 x 812 x 812 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 26x51 GPU FDTD loopiter = 1dimx = 812gpu_time = 9195.060015fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 937018==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10248693103.79,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236415852.76,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,15547792,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.39,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6951840,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.75,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.32,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,14739002.95,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.8,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,328478593293.29,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.95,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.17,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.83,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.32,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.32,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2529552.19,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1295130720,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2529642.16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1295176787,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Block Size,,512,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Grid Size,,1326,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Threads,thread,678912,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Waves Per SM,,10.36,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",937018,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
800,==PROF== Connected to process 938507 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 804 x 804 x 804 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 26x51 GPU FDTD loopiter = 1dimx = 804gpu_time = 9232.697964fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 938507==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10241759143.14,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234857644.66,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,15113000,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.04,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.53,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6762080,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.56,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.36,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,14359602.45,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.04,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.45,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.09,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,329687263090.65,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.45,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.04,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.57,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.88,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.04,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.42,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.58,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.94,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.94,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 22.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2505145.5,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1282634496,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2505235.11,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1282680375,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Block Size,,512,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Grid Size,,1326,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Threads,thread,678912,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Launch Statistics,Waves Per SM,,10.36,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(26, 51, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",938507,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
792,==PROF== Connected to process 939977 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 796 x 796 x 796 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 25x50 GPU FDTD loopiter = 1dimx = 796gpu_time = 8531.676054fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 939977==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10200507447.29,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2225878381.04,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,13719042,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.42,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6163136,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.43,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.66,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,13438284.23,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.05,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,346814089450.57,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.14,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.8,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.99,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.42,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.58,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.96,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.97,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2338554.69,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1197340000,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2338643.33,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1197385386,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Block Size,,512,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Grid Size,,1250,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Threads,thread,640000,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Waves Per SM,,9.77,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",939977,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
784,==PROF== Connected to process 941363 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 788 x 788 x 788 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 25x50 GPU FDTD loopiter = 1dimx = 788gpu_time = 8652.947903fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 941363==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10253676470.59,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237501812.95,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,13711068,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.94,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.55,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6127616,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.22,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.2,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,13359413.36,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.94,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.33,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.97,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,340122326203.21,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.9,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.94,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.77,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.03,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.94,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.31,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.69,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.11,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.11,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2315546.88,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1185560000,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2315635.41,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1185605330,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Block Size,,512,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Grid Size,,1250,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Threads,thread,640000,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Launch Statistics,Waves Per SM,,9.77,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 50, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",941363,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
776,==PROF== Connected to process 942750 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 780 x 780 x 780 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 25x49 GPU FDTD loopiter = 1dimx = 780gpu_time = 8293.127060fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 942750==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10260227773.62,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2238950310.11,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,13979044,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.56,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.8,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,6243392,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.6,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.09,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,13394177.65,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.56,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.77,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.39,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,323083595583.94,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.69,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.56,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.78,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.15,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.56,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.77,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.23,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.85,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.85,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2246688.28,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1150304400,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2246775.67,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1150349142,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Block Size,,512,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Grid Size,,1225,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Threads,thread,627200,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Waves Per SM,,9.57,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",942750,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
768,==PROF== Connected to process 944080 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 772 x 772 x 772 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 25x49 GPU FDTD loopiter = 1dimx = 772gpu_time = 8204.270124fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 944080==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10230906829.27,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232536567.92,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,13286663,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.49,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.6,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5951168,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.48,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.72,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,12762345.82,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.49,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.43,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.06,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,329996515641.97,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.67,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.49,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.95,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.2,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.49,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.41,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.59,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.99,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.99,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2224140.62,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1138760000,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2224229.16,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1138805331,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Block Size,,512,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Grid Size,,1225,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Threads,thread,627200,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Launch Statistics,Waves Per SM,,9.57,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(25, 49, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",944080,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
760,==PROF== Connected to process 945373 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 764 x 764 x 764 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 24x48 GPU FDTD loopiter = 1dimx = 764gpu_time = 7703.649044fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 945373==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10232345332.86,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232830646.56,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,12181344,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.23,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,35.09,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5455328,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.32,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.45,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,11916947.73,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.23,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.37,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.01,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,344698432064.95,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.05,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.23,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.87,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.14,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.23,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.38,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.62,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.05,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.05,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2070396,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1060042752,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2070480.63,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1060086082,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Block Size,,512,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Grid Size,,1152,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Threads,thread,589824,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Waves Per SM,,9,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",945373,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
752,==PROF== Connected to process 946604 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 756 x 756 x 756 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 24x48 GPU FDTD loopiter = 1dimx = 756gpu_time = 7557.765961fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 946604==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10286902583.36,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2244738575.87,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,12090395,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.41,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5385856,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.28,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.12,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,11793104.91,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.34,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.99,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,339849775411.75,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.96,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.86,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.2,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.35,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.65,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.05,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,2044584,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1046827008,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,2044668.76,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1046870405,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Block Size,,512,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Grid Size,,1152,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Threads,thread,589824,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Launch Statistics,Waves Per SM,,9,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 48, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",946604,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
744,==PROF== Connected to process 947827 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 748 x 748 x 748 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 24x47 GPU FDTD loopiter = 1dimx = 748gpu_time = 7581.418037fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 947827==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10252775352.76,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237260313.96,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,11563584,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.76,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5168352,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.85,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,11298444.75,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.55,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.2,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,342085480245.93,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.29,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.51,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.49,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.69,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.55,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.45,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.79,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.79,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1982530.5,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1015055616,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1982614.09,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1015098412,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Block Size,,512,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Grid Size,,1128,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Threads,thread,577536,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.81,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",947827,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
736,==PROF== Connected to process 948997 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 740 x 740 x 740 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 24x47 GPU FDTD loopiter = 1dimx = 740gpu_time = 7423.194885fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 948997==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10232220049.66,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232809617.89,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,11730299,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.46,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.3,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5253408,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.73,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.91,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,11430228.62,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.46,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.16,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.8,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,327123098757.99,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.66,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.46,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.29,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.59,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.46,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.16,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.84,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.32,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.32,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1961768.25,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,1004425344,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1961851.92,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,1004468184,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Block Size,,512,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Grid Size,,1128,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Threads,thread,577536,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.81,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(24, 47, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",948997,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
728,==PROF== Connected to process 950150 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 732 x 732 x 732 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 23x46 GPU FDTD loopiter = 1dimx = 732gpu_time = 6981.531143fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 950150==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10251233218.6,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236954059.51,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,11396713,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.28,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.08,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5094528,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,49.94,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.05,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,10562648.77,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.28,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.24,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.87,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,325534056933.24,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.55,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.28,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.99,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.02,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.28,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.28,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.72,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.19,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.19,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.1% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1820553.5,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,932123392,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1820635.18,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,932165211,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Block Size,,512,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Grid Size,,1058,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Threads,thread,541696,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.27,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",950150,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
720,==PROF== Connected to process 951257 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 724 x 724 x 724 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 23x46 GPU FDTD loopiter = 1dimx = 724gpu_time = 6858.078957fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 951257==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10236037660.74,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233656014.53,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,11732028,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.47,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.28,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5252224,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,47.77,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.02,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,10921595.26,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.47,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.66,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.49,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.66,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.09,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,307338386176.98,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.63,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.47,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.59,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.15,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.47,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.5,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.16,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.5,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,24.2,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,24.2,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.2% of the total average of 24.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1801079.69,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,922152800,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1801161.82,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,922194850,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Block Size,,512,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Grid Size,,1058,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Threads,thread,541696,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.27,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 46, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",951257,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
712,==PROF== Connected to process 952332 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 716 x 716 x 716 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 23x45 GPU FDTD loopiter = 1dimx = 716gpu_time = 6684.222937fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 952332==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10223988597.26,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230999867.04,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,10886208,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.37,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.36,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,4879296,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.85,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.86,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,9927699.81,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.37,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.56,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.2,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,317608232007.24,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.6,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.37,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.18,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.27,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.37,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.57,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.43,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.79,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.79,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1742875.31,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,892352160,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1742955.81,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,892393374,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Block Size,,512,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Grid Size,,1035,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Threads,thread,529920,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.09,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",952332,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
704,==PROF== Connected to process 953380 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 708 x 708 x 708 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 23x45 GPU FDTD loopiter = 1dimx = 708gpu_time = 6870.645046fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 953380==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10247173648.17,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236104829.5,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,11236950,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.43,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.43,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,5025088,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,48.67,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.62,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,10257607.3,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.43,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.67,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,16.81,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.67,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.42,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,299343301450.64,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.6,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.43,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.25,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.34,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.43,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,16.73,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,No Eligible,%,83.27,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.35,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 6.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.89,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.89,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 8.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1723824.84,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,882598320,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1723906.23,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,882639990,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Block Size,,512,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Grid Size,,1035,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Threads,thread,529920,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Launch Statistics,Waves Per SM,,8.09,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(23, 45, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",953380,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
696,==PROF== Connected to process 954409 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 700 x 700 x 700 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 22x44 GPU FDTD loopiter = 1dimx = 700gpu_time = 6256.761789fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 954409==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10220561823.4,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230247191.14,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,9590291,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.14,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.18,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,4299904,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.62,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.97,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,9119858.27,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.14,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.48,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.11,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,335374512547.26,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.5,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.14,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.67,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.2,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.14,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.5,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.5,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.86,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.86,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1594417,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,816341504,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1594495.82,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,816381860,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Block Size,,512,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Grid Size,,968,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Threads,thread,495616,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Waves Per SM,,7.56,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",954409,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
688,==PROF== Connected to process 955389 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 692 x 692 x 692 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 22x44 GPU FDTD loopiter = 1dimx = 692gpu_time = 6220.751047fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 955389==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10254245453.45,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237625681.91,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,9512778,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.99,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.49,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,4251136,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.68,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.67,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,9006491.25,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.99,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.51,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.14,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,329643050704.56,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.42,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.99,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.51,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.31,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.99,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.52,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.48,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.87,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.87,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 22.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1576599.75,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,807219072,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1576678.74,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,807259517,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Block Size,,512,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Grid Size,,968,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Threads,thread,495616,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Launch Statistics,Waves Per SM,,7.56,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 44, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",955389,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
680,==PROF== Connected to process 956352 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 684 x 684 x 684 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 22x43 GPU FDTD loopiter = 1dimx = 684gpu_time = 6093.338966fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 956352==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10239331777.92,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234359699.22,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,9439183,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.68,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.45,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,4224384,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.08,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.87,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,8798712.52,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.68,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.27,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.92,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,318953943580.89,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.75,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.68,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.64,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.16,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.68,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.31,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.69,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.15,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.15,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 23.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1519571.62,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,778020672,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1519651.89,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,778061766,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Block Size,,512,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Grid Size,,946,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Threads,thread,484352,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Waves Per SM,,7.39,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",956352,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
672,==PROF== Connected to process 957295 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 676 x 676 x 676 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 22x43 GPU FDTD loopiter = 1dimx = 676gpu_time = 6108.883142fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 957295==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10238425659.7,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234167864.67,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,9324900,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.7,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.91,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,4173600,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.21,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.85,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,8673522.1,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.7,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.33,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.97,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,313630730304.77,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.76,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.7,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.89,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.47,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.7,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.33,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.67,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.14,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.14,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.82,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1503253.12,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,769665600,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1503331.53,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,769705741,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Block Size,,512,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Grid Size,,946,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Threads,thread,484352,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Launch Statistics,Waves Per SM,,7.39,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(22, 43, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",957295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
664,==PROF== Connected to process 958199 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 668 x 668 x 668 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 21x42 GPU FDTD loopiter = 1dimx = 668gpu_time = 5649.435997fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 958199==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10227670097.42,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2231793319.59,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,8139768,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.3,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.78,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3647008,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.29,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.7,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,7979063.03,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.3,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.36,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,341474471128.11,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.09,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.3,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.54,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.42,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.3,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.37,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.63,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.05,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.05,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 34.0% of the total average of 23.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1385318.81,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,709283232,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1385395.07,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,709322277,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Block Size,,512,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Grid Size,,882,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Threads,thread,451584,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.89,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",958199,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
656,==PROF== Connected to process 959065 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 660 x 660 x 660 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 21x42 GPU FDTD loopiter = 1dimx = 660gpu_time = 5872.564077fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 959065==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10216348611.85,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2229327838.98,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,8000260,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.56,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.29,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3588480,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.75,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.52,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,7812902.59,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.56,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.52,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.16,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,336352630640.27,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.22,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.56,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.6,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.46,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.56,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.53,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.47,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.84,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.84,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1369084.5,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,700971264,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1369160.99,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,701010428,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Block Size,,512,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Grid Size,,882,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Threads,thread,451584,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.89,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 42, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",959065,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
648,==PROF== Connected to process 959919 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 652 x 652 x 652 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 21x41 GPU FDTD loopiter = 1dimx = 652gpu_time = 5449.328899fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 959919==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10244608204.37,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235479463.6,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7766282,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.24,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.91,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3473920,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.35,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.29,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,7446481.15,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.24,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.71,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.74,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.71,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.38,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,333541525423.73,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.06,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.24,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.14,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.48,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.24,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.74,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.26,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.55,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.55,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1320639.47,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,676167408,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1320716.58,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,676206887,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Block Size,,512,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Grid Size,,861,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Threads,thread,440832,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.73,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",959919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
640,==PROF== Connected to process 960731 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 644 x 644 x 644 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 21x41 GPU FDTD loopiter = 1dimx = 644gpu_time = 5636.655092fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 960731==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10261666513.1,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2239233496.39,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7777054,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,48.58,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.84,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3472960,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.78,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.65,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,7439823.23,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,48.58,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.67,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.54,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.17,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,323562369851.65,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.72,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,48.58,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.08,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.56,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,48.58,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.54,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.46,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.84,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.84,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.7% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1304791.69,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,668053344,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1304869.14,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,668093002,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Block Size,,512,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Grid Size,,861,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Threads,thread,440832,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.73,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(21, 41, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",960731,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
632,==PROF== Connected to process 961543 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 636 x 636 x 636 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 20x40 GPU FDTD loopiter = 1dimx = 636gpu_time = 5114.160061fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 961543==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10247005151.95,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236021255.45,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7601909,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.61,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.09,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3399616,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.26,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.58,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,6897769.64,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.61,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.36,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.98,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,315713172311.23,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.21,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.61,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.1,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.24,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.61,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.37,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.63,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.1,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.1,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1197625,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,613184000,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1197699.88,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,613222337,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Block Size,,512,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Grid Size,,800,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Threads,thread,409600,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.25,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",961543,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
624,==PROF== Connected to process 962311 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 628 x 628 x 628 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 20x40 GPU FDTD loopiter = 1dimx = 628gpu_time = 5155.207872fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 962311==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10258693734.87,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2238559966.46,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7445624,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.98,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.72,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3325920,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.81,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.53,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,6738006.81,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.98,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.56,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.18,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,312402751719.82,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.4,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.98,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.91,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.38,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.98,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.58,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.42,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.78,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.78,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1182900,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,605644800,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1182975.22,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,605683314,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Block Size,,512,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Grid Size,,800,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Threads,thread,409600,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.25,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 40, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",962311,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
616,==PROF== Connected to process 963055 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 620 x 620 x 620 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 20x39 GPU FDTD loopiter = 1dimx = 620gpu_time = 4971.221924fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 963055==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10198139112.23,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2225319823.25,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7209915,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.72,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.22,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3239744,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.3,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.56,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,6425244.16,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.72,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.71,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.73,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.71,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.36,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,305694079532.21,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.26,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.72,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.96,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.66,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.72,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.71,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.29,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.52,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.52,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.6% of the total average of 22.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1138970.62,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,583152960,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1139044.03,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,583190543,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Block Size,,512,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Grid Size,,780,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Threads,thread,399360,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.09,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",963055,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
608,==PROF== Connected to process 963774 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 612 x 612 x 612 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 20x39 GPU FDTD loopiter = 1dimx = 612gpu_time = 4949.285984fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 963774==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10259086797.31,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2238644556.84,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,7154049,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.44,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.47,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,3195552,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.04,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.21,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,6368514.15,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.44,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.61,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.27,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,300069416488.92,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.12,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.44,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.79,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.79,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.44,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.61,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.39,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.64,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.64,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.8,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1121493.75,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,574204800,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1121566.78,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,574242192,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Block Size,,512,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Grid Size,,780,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Threads,thread,399360,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Launch Statistics,Waves Per SM,,6.09,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(20, 39, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",963774,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
600,==PROF== Connected to process 964482 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 604 x 604 x 604 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 19x38 GPU FDTD loopiter = 1dimx = 604gpu_time = 4616.644859fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 964482==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10215807813.06,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2229212884.22,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,6203347,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.88,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.45,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2782624,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.71,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.84,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,5856527.09,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.88,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.51,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.15,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,328097565463.39,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.37,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.88,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.05,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.34,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.88,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.55,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.45,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.79,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.79,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 22.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1025646.12,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,525130816,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1025717.58,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,525167401,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Block Size,,512,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Grid Size,,722,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Threads,thread,369664,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.64,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",964482,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
592,==PROF== Connected to process 965158 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 596 x 596 x 596 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 19x38 GPU FDTD loopiter = 1dimx = 596gpu_time = 4285.799980fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 965158==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10274633806.09,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2242043968.37,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,6171849,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.49,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.51,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2752640,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.54,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.35,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,5799006.02,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.49,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.46,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.09,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,320685096489.19,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.17,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.49,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.98,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.42,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.49,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.51,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.49,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.87,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.87,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,1012356.81,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,518326688,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,1012428.48,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,518363382,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Block Size,,512,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Grid Size,,722,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Threads,thread,369664,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.64,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 38, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",965158,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
584,==PROF== Connected to process 965798 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 588 x 588 x 588 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 19x37 GPU FDTD loopiter = 1dimx = 588gpu_time = 4509.696960fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 965798==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10201917436.13,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2226119316.4,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,5909956,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.65,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.43,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2654656,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.82,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.52,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,5433637.69,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.65,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.9,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.54,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,317655536536.56,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.25,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.65,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.01,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.65,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.65,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.89,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.11,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.32,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.32,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.6% of the total average of 22.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,972776.25,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,498061440,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,972846.79,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,498097558,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Block Size,,512,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Grid Size,,703,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Threads,thread,359936,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.49,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",965798,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
576,==PROF== Connected to process 966430 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 580 x 580 x 580 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 19x37 GPU FDTD loopiter = 1dimx = 580gpu_time = 4538.097858fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 966430==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10285524067.01,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2244367103.95,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,5897546,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.11,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.37,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2627552,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.28,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.86,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,5416809.19,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.11,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.71,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.72,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.71,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.35,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,309728830485.56,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.97,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.11,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.22,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.72,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.11,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.73,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.27,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.54,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.54,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.82,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.6% of the total average of 22.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,959836.66,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,491436368,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,959907.32,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,491472550,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Block Size,,512,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Grid Size,,703,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Threads,thread,359936,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.49,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(19, 37, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",966430,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
568,==PROF== Connected to process 967048 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 572 x 572 x 572 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 18x36 GPU FDTD loopiter = 1dimx = 572gpu_time = 4178.133011fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 967048==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10243069333.7,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235101525.3,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,5732151,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.06,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.52,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2564448,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.84,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.87,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4967631.2,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.06,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.57,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.19,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,300093038345.87,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.42,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.06,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.92,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.49,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.06,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.61,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.39,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.66,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.66,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.9% of the total average of 22.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,872815.5,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,446881536,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,872884.96,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,446917098,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Block Size,,512,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Grid Size,,648,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Threads,thread,331776,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.06,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",967048,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
560,==PROF== Connected to process 967638 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 564 x 564 x 564 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 18x36 GPU FDTD loopiter = 1dimx = 564gpu_time = 4342.875957fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 967638==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10235407342.03,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233457072.78,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,5668993,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,43.94,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,29.76,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2538080,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.86,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.29,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4896818.34,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,43.94,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.58,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.2,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,292450104015.63,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.36,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,43.94,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.73,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.33,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,43.94,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.61,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.39,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.73,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.73,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.7% of the total average of 22.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,860888.25,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,440774784,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,860957.05,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,440810010,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Block Size,,512,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Grid Size,,648,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Threads,thread,331776,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Launch Statistics,Waves Per SM,,5.06,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 36, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",967638,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
552,==PROF== Connected to process 968214 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 556 x 556 x 556 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 18x35 GPU FDTD loopiter = 1dimx = 556gpu_time = 4004.903078fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 968214==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10212355307.41,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2228370636.39,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4827921,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.45,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,33.18,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2166432,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.5,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.37,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4727959.05,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.45,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.7,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.46,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.7,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.07,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,325271295844.97,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.16,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.45,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.03,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.85,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.45,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.48,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.52,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.88,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.88,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 22.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,825378.75,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,422593920,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,825447.73,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,422629239,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Block Size,,512,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Grid Size,,630,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Threads,thread,322560,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.92,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",968214,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
544,==PROF== Connected to process 968767 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 548 x 548 x 548 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 18x35 GPU FDTD loopiter = 1dimx = 548gpu_time = 3888.725042fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 968767==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10248192917.45,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2236213747.91,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4712550,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.94,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.69,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2107264,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.33,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.19,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4585115.98,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.94,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.71,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.69,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.75,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.71,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.37,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,321601287736.14,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.41,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.94,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.27,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.87,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.94,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 84.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.77,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.23,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.49,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.49,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.5% of the total average of 22.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,813782.81,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,416656800,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,813851.92,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,416692184,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Block Size,,512,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Grid Size,,630,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Threads,thread,322560,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.92,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(18, 35, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",968767,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
536,==PROF== Connected to process 969295 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 540 x 540 x 540 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 17x34 GPU FDTD loopiter = 1dimx = 540gpu_time = 3780.569077fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 969295==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10228609320.71,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2231953818.44,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4488121,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.36,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.6,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,2010720,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.07,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.67,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4081959.26,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.36,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.97,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.63,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,320092241585.1,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.11,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.36,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.8,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.52,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.36,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.24,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.24,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.5% of the total average of 22.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,733662.62,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,375635264,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,733730.26,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,375669891,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Block Size,,512,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Grid Size,,578,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Threads,thread,295936,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.52,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",969295,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
528,==PROF== Connected to process 969806 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 532 x 532 x 532 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 17x34 GPU FDTD loopiter = 1dimx = 532gpu_time = 3702.544928fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 969806==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10221520482.91,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230364186.35,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4430232,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.28,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,31.8,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1986176,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.24,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.29,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,4009053.02,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.28,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.65,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.05,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.69,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,312014886898.24,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.06,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.28,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.76,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.66,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.28,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.05,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.95,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.19,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.19,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.4% of the total average of 22.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,723692.12,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,370530368,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,723759.99,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,370565115,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Block Size,,512,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Grid Size,,578,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Threads,thread,295936,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.52,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 34, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",969806,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
520,==PROF== Connected to process 970304 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 524 x 524 x 524 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 17x33 GPU FDTD loopiter = 1dimx = 524gpu_time = 3489.087105fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 970304==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10259230493.82,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2238606881.84,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4335333,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.19,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.76,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1936480,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.22,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.95,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3834857.33,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.19,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.05,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.69,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,302996215814.26,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.51,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.19,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.43,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.04,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.19,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.98,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.13,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.13,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.7% of the total average of 22.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,692081.16,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,354345552,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,692146.85,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,354379187,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Block Size,,512,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Grid Size,,561,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Threads,thread,287232,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.38,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",970304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
512,==PROF== Connected to process 970775 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 516 x 516 x 516 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 17x33 GPU FDTD loopiter = 1dimx = 516gpu_time = 3661.860943fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 970775==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10214127113.98,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2228753629.22,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,4272301,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.17,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1916768,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.16,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.56,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3781059.27,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.17,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.03,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.67,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,294164844154.33,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.49,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.17,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.48,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.14,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.17,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.06,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.94,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.14,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.14,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.81,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.5% of the total average of 22.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,681755.25,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,349058688,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,681823.36,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,349093560,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Block Size,,512,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Grid Size,,561,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Threads,thread,287232,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Launch Statistics,Waves Per SM,,4.38,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(17, 33, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",970775,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
504,==PROF== Connected to process 971242 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 508 x 508 x 508 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 16x32 GPU FDTD loopiter = 1dimx = 508gpu_time = 3413.434029fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 971242==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10229554496.45,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232055543.37,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3507149,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.54,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,34.12,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1571136,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,51.66,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,25.03,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3430792.13,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.54,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.71,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.86,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.71,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.49,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,335040286773.39,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.72,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.54,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.09,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.81,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.54,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.79,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.21,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.37,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.43,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.43,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.6% of the total average of 22.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,612784,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,313745408,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,612848.38,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,313778368,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Block Size,,512,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Grid Size,,512,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Threads,thread,262144,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Waves Per SM,,4,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",971242,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
496,==PROF== Connected to process 971676 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 500 x 500 x 500 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 16x32 GPU FDTD loopiter = 1dimx = 500gpu_time = 3444.555044fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 971676==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10256435097.35,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2238038950.49,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3548546,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,49.17,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.29,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1585472,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,50.07,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.02,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3484516.34,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,49.17,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.69,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.68,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,17.32,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.69,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,17.92,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,317909175311.83,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.02,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,49.17,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.92,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.99,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,49.17,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,17.33,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.17,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,No Eligible,%,82.67,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.36,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,23.08,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,23.08,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.8% of the total average of 23.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,603360,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,308920320,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,603424.4,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,308953295,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Block Size,,512,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Grid Size,,512,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Threads,thread,262144,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Launch Statistics,Waves Per SM,,4,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 32, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",971676,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
488,==PROF== Connected to process 972108 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 492 x 492 x 492 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 16x31 GPU FDTD loopiter = 1dimx = 492gpu_time = 3217.634916fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 972108==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10229303775.55,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232007042.74,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3299514,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.42,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.74,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1478144,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.72,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,25.12,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3155031.39,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.42,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.73,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.24,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.73,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.87,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,321546501558.71,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.66,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.42,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.07,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.51,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.42,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.25,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.75,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.93,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.94,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.0% of the total average of 21.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,575375.5,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,294592256,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,575440.14,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,294625352,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Block Size,,512,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Grid Size,,496,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Threads,thread,253952,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.88,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",972108,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
480,==PROF== Connected to process 972517 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 484 x 484 x 484 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 16x31 GPU FDTD loopiter = 1dimx = 484gpu_time = 3159.248114fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 972517==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10244108305.01,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235240852.25,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3220548,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.82,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.07,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1440672,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,53.12,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.75,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,3080923.23,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.82,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.74,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.38,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.74,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.01,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,315365807067.81,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.86,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.82,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.63,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.46,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.82,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.38,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.62,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.39,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.76,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.76,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.3% of the total average of 21.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,566246,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,289917952,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,566309.6,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,289950516,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Block Size,,512,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Grid Size,,496,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Threads,thread,253952,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.88,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(16, 31, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",972517,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
472,==PROF== Connected to process 972910 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 476 x 476 x 476 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 15x30 GPU FDTD loopiter = 1dimx = 476gpu_time = 3196.897984fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 972910==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10239256049.12,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234215910.63,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3167745,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.11,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.86,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1417728,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.02,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2808567.3,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.11,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.72,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.72,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.61,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,303353647526.18,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.47,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.11,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.8,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.71,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.11,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.5 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.09,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.91,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,22.11,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,22.12,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.94,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.4% of the total average of 22.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,505448.44,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,258789600,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,505511.55,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,258821916,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Block Size,,512,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Grid Size,,450,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Threads,thread,230400,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.52,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",972910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
464,==PROF== Connected to process 973304 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 468 x 468 x 468 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 15x30 GPU FDTD loopiter = 1dimx = 468gpu_time = 3049.525023fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 973304==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10229820532.45,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232105152.58,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,3079456,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.58,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.44,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1379488,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.87,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.87,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2713190.37,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.58,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.73,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.26,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.73,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.92,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,298891925120.04,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.71,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.58,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.88,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.82,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.58,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.34,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.66,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.82,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.82,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.4% of the total average of 21.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,495365.62,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,253627200,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,495429.49,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,253659900,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Block Size,,512,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Grid Size,,450,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Threads,thread,230400,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.52,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 30, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",973304,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
456,==PROF== Connected to process 973680 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 460 x 460 x 460 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 15x29 GPU FDTD loopiter = 1dimx = 460gpu_time = 2834.604025fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 973680==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10206852538.79,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2227031435.27,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2978097,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.77,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,29.53,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1337120,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,53.26,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.5,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2559188.86,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.77,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.74,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.42,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.74,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.06,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,289310709584.78,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.3,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.77,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.18,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.03,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.77,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.47,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.53,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.39,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.7,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.7,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.1% of the total average of 21.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,471349.69,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,241331040,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,471412.55,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,241363228,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Block Size,,512,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Grid Size,,435,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Threads,thread,222720,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.4,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",973680,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
448,==PROF== Connected to process 974031 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 452 x 452 x 452 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 15x29 GPU FDTD loopiter = 1dimx = 452gpu_time = 2818.639040fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 974031==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10235975668.63,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233452260,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2961121,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.24,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,28.3,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1325696,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.81,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.87,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2536446.85,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.24,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.73,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.27,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.73,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.9,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,278047214444.34,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.02,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.24,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.1,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.22,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.24,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.26,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.74,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.98,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.82,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.82,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.79,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.3% of the total average of 21.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,463342.97,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,237231600,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,463406.34,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,237264046,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Block Size,,512,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Grid Size,,435,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Threads,thread,222720,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.4,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(15, 29, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",974031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
440,==PROF== Connected to process 974371 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 444 x 444 x 444 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 14x28 GPU FDTD loopiter = 1dimx = 444gpu_time = 2777.661085fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 974371==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10208873559.75,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2227249339.71,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2831348,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,41.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,27.69,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1271072,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,52.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.6,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2242471.44,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,41.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.73,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.58,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.3,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.73,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.93,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,271408373404.5,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.32,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,41.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.66,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.56,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,41.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.32,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.68,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.38,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.89,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.9,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,410326,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,210086912,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,410388.22,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,210118771,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Block Size,,512,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Grid Size,,392,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Threads,thread,200704,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.06,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",974371,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
432,==PROF== Connected to process 974702 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 436 x 436 x 436 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 14x28 GPU FDTD loopiter = 1dimx = 436gpu_time = 2826.457024fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 974702==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10234543340.65,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233138630.2,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2766141,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,42.11,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,26.77,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,1238560,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,53.52,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.57,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2176468.14,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,42.11,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.74,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.58,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.52,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.74,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.15,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,263055005813.2,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.44,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,42.11,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.77,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.91,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,42.11,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.55,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.45,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.39,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.6,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.6,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,403110.75,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,206392704,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,403171.94,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,206424031,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Block Size,,512,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Grid Size,,392,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Threads,thread,200704,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Launch Statistics,Waves Per SM,,3.06,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 28, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",974702,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
424,==PROF== Connected to process 975031 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 428 x 428 x 428 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 14x27 GPU FDTD loopiter = 1dimx = 428gpu_time = 2657.848835fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 975031==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10213699116.62,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2228492948.61,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2142184,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,51.49,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,32.28,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,961152,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,53.26,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,25.28,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2070540.8,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,51.49,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.74,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.71,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.44,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.74,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.06,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,316467838593.69,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,26.21,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,51.49,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.17,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.24,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,51.49,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.49,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.51,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.39,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.71,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.71,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,381756.38,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,195459264,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,381817.19,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,195490402,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Block Size,,512,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Grid Size,,378,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Threads,thread,193536,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.95,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",975031,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
416,==PROF== Connected to process 975342 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 420 x 420 x 420 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 14x27 GPU FDTD loopiter = 1dimx = 420gpu_time = 2777.237892fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 975342==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10302594917.59,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2247985833.7,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,2143089,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.51,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.64,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,953248,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,53.04,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.47,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,2040603.91,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.51,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.73,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.37,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.73,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,18.98,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,303081137332.57,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.71,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.51,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.72,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.48,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.51,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.38,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.18,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.62,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.39,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.85,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.85,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.81,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.3% of the total average of 21.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,374798.81,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,191896992,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,374859.54,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,191928087,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Block Size,,512,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Grid Size,,378,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Threads,thread,193536,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.95,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(14, 27, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",975342,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
408,==PROF== Connected to process 975636 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 412 x 412 x 412 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 13x26 GPU FDTD loopiter = 1dimx = 412gpu_time = 2303.300142fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 975636==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10268208037.15,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2240414405.15,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1997179,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.55,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.56,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,891328,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.19,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.76,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1752326.2,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.55,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.75,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.77,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.75,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.39,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,301291735477.85,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.21,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.55,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.74,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.87,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.55,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.83,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.17,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.2,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.2,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.95,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.1% of the total average of 21.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,328916.25,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,168405120,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,328976.27,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,168435852,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Block Size,,512,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Grid Size,,338,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Threads,thread,173056,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.64,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",975636,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
400,==PROF== Connected to process 975907 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 404 x 404 x 404 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 13x26 GPU FDTD loopiter = 1dimx = 404gpu_time = 2368.559837fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 975907==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10190887012,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2223564829.61,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1963157,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,47.45,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,29.46,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,882784,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.51,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.13,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1708477.48,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,47.45,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.66,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.89,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.51,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,288256207634.03,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,24.15,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,47.45,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.13,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.93,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,47.45,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.87,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.13,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.2,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.21,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.0% of the total average of 21.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,322694.94,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,165219808,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,322753.99,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,165250042,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Block Size,,512,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Grid Size,,338,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Threads,thread,173056,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.64,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 26, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",975907,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
392,==PROF== Connected to process 976181 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 396 x 396 x 396 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 13x25 GPU FDTD loopiter = 1dimx = 396gpu_time = 2292.729855fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 976181==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10231111447.19,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232348748.36,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1889609,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,46.4,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,28.38,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,846368,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,55.02,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.72,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1593482.72,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,46.4,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.02,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.69,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,278700442360.77,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.62,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,46.4,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.34,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.25,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,46.4,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 84.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.93,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.07,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.08,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.09,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,303001.56,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,155136800,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,303061.76,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,155167621,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Block Size,,512,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Grid Size,,325,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Threads,thread,166400,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.54,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",976181,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
384,==PROF== Connected to process 976427 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 388 x 388 x 388 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 13x25 GPU FDTD loopiter = 1dimx = 388gpu_time = 2209.444046fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 976427==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10235259666.68,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233262280.77,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1871197,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.92,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,27.18,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,837792,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.65,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.15,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1572171.09,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.92,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.64,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.92,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.56,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,267071998777.74,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.37,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.92,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.88,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.61,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.92,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 83.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.85,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.15,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.22,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.23,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.77,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.0% of the total average of 21.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,297395.31,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,152266400,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,297454.93,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,152296922,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Block Size,,512,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Grid Size,,325,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Threads,thread,166400,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.54,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(13, 25, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",976427,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
376,==PROF== Connected to process 976677 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 380 x 380 x 380 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 12x24 GPU FDTD loopiter = 1dimx = 380gpu_time = 2263.734102fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 976677==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10224156518.27,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230672176.45,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1787969,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,41.72,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,25.91,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,801440,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.65,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.7,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1364754.91,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,41.72,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.58,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.93,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.56,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,254310081852.67,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.24,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,41.72,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.89,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.9,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,41.72,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,18.91,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,No Eligible,%,81.09,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21.12,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.13,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,258237,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,132217344,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,258296.34,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,132247726,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Block Size,,512,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Grid Size,,288,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Threads,thread,147456,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.25,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",976677,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
368,==PROF== Connected to process 976919 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 372 x 372 x 372 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 12x24 GPU FDTD loopiter = 1dimx = 372gpu_time = 2246.476889fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 976919==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10199556060.85,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2225355678.87,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1753988,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,41.64,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,24.91,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,788096,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.76,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.28,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1333461.71,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,41.64,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.58,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,18.97,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.6,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,243902387526.39,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.2,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,41.64,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.62,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.16,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,41.64,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.02,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.98,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21.01,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 7.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.2% of the total average of 21.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,252936,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,129503232,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,252994.22,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,129533042,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Block Size,,512,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Grid Size,,288,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Threads,thread,147456,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.25,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 24, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",976919,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
360,==PROF== Connected to process 977159 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 364 x 364 x 364 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 12x23 GPU FDTD loopiter = 1dimx = 364gpu_time = 2246.088982fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 977159==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10177876738.23,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2220652095.04,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1691635,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,40.49,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,23.94,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,761696,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,54.85,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,19.93,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1248677.47,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,40.49,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.76,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.56,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.01,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.76,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.63,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,233874217535.6,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,20.61,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,40.49,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.57,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.57,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,40.49,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.01,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.99,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.4,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.40 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,21,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,21,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.86,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.0% of the total average of 21.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,237316.88,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,121506240,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,237375.27,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,121536137,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Block Size,,512,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Grid Size,,276,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Threads,thread,141312,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.16,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",977159,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
352,==PROF== Connected to process 977397 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 356 x 356 x 356 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 12x23 GPU FDTD loopiter = 1dimx = 356gpu_time = 2256.992817fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 977397==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10238624727.79,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233924581.99,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1652435,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,40.55,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,22.59,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,739616,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,55.45,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,19.39,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1208317.45,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,40.55,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.77,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.56,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.22,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.77,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,19.84,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,222035564400.99,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,20.64,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,40.55,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.02,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.7,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,40.55,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 83.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.29,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.71,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.41,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.41 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,20.74,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,20.74,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.79,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 33.0% of the total average of 20.7 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,232236.75,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,118905216,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,232295.27,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,118935176,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Block Size,,512,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Grid Size,,276,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Threads,thread,141312,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Launch Statistics,Waves Per SM,,2.16,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(12, 23, 1)",0,8.9,Occupancy,,,,Occupancy,WRN,"This kernel's theoretical occupancy (33.3%) is limited by the number of required registers. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.""0",977397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
344,==PROF== Connected to process 977622 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 348 x 348 x 348 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 11x22 GPU FDTD loopiter = 1dimx = 348gpu_time = 1946.918011fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 977622==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10263597612.96,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2239204908.74,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1120855,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,51.25,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,30.23,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,500480,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,55.91,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,25.17,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,1027262.83,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,51.25,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.78,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.71,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.39,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.78,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,20.01,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,297810741687.98,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,26.1,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,51.25,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.71,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.81,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,51.25,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.37,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.19,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.63,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.41,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.2 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.41 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,20.64,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,20.65,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.95,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.9% of the total average of 20.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,199173.56,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,101976864,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,199230.88,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,102006209,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Block Size,,512,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Grid Size,,242,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Threads,thread,123904,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.89,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,977622,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
336,==PROF== Connected to process 977821 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 340 x 340 x 340 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 11x22 GPU FDTD loopiter = 1dimx = 340gpu_time = 2024.678946fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 977821==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10245843696.82,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235243561.75,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1092991,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,51.36,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,28.62,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,488896,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,56.27,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.82,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,997473.13,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,51.36,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.78,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.71,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.53,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.78,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,20.14,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,281503861762.01,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,26.15,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,51.36,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.97,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.36,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,51.36,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.52,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.2,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.48,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.41,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.41 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,20.5,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,20.51,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.9% of the total average of 20.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,194719.25,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,99696256,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,194775.49,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,99725049,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Block Size,,512,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Grid Size,,242,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Threads,thread,123904,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.89,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 22, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,977821,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
328,==PROF== Connected to process 978021 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 332 x 332 x 332 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 11x21 GPU FDTD loopiter = 1dimx = 332gpu_time = 2038.536072fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978021==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10165285861.01,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2217779753.2,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1036681,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.49,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,27.5,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,467360,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,57.09,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.55,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,916642.56,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.49,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.79,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.82,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.79,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,20.43,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,268364806573.09,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.71,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.49,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.95,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.82,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.49,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.84,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.2,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.16,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.42,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.42 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,20.2,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,20.2,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.89,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.6% of the total average of 20.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,181616.53,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,92987664,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,181673.3,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,93016728,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Grid Size,,231,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Threads,thread,118272,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.8,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978021,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
320,==PROF== Connected to process 978207 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 324 x 324 x 324 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 11x21 GPU FDTD loopiter = 1dimx = 324gpu_time = 1913.825035fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978207==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10218171461.12,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2229239759.46,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,1001994,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,50.9,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,25.99,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,449408,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,57.7,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,23.86,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,883812.51,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,50.9,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.8,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.7,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,19.97,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.8,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,20.65,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,254911706066.65,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,25.91,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,50.9,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.97,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.03,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,50.9,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 82.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,19.94,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.2,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,No Eligible,%,80.06,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.42,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 5.0 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.42 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,20.06,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,20.06,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.74,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.4% of the total average of 20.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,176440.69,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,90337632,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,176497.21,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,90366572,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Grid Size,,231,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Threads,thread,118272,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.8,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(11, 21, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978207,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
312,==PROF== Connected to process 978397 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 316 x 316 x 316 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 10x20 GPU FDTD loopiter = 1dimx = 316gpu_time = 1974.756002fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978397==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10247565236.79,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2235767997.99,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,962470,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.76,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,24.19,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,430432,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,58.11,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.99,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,741124.19,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.76,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.81,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.62,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.15,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.81,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,20.8,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,237982008772.58,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.79,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.76,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.96,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.37,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.76,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.21,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.2,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.79,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.02,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.42,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.02 active warps per scheduler, but only an average of 0.42 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.88,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.89,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.91,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.5 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.7% of the total average of 19.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,149312.5,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,76448000,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,149367.26,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,76476036,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Grid Size,,200,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Threads,thread,102400,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.56,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978397,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
304,==PROF== Connected to process 978584 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 308 x 308 x 308 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 10x20 GPU FDTD loopiter = 1dimx = 308gpu_time = 1825.099945fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978584==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10207110697.1,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2226804314.69,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,918085,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,45.74,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,22.98,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,412224,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,58.69,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.97,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,715472.13,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,45.74,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.81,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.63,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.36,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.81,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,225203850333.8,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,23.29,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,45.74,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.58,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.08,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,45.74,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.37,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.2,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.63,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.43,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.63,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.63,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.4 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.4% of the total average of 19.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,145631.25,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,74563200,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,145688.28,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,74592399,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Grid Size,,200,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Threads,thread,102400,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.56,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 20, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978584,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
296,==PROF== Connected to process 978746 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 300 x 300 x 300 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 10x19 GPU FDTD loopiter = 1dimx = 300gpu_time = 1753.799915fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978746==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10149516831.26,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2214273011.45,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,889831,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,43.68,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,21.18,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,401792,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,59.09,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,21.01,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,657745.97,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,43.68,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.82,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.51,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.82,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.14,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,206323988531.38,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.24,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,43.68,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.43,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.05,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,43.68,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.5,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.5,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.43,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.54,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.55,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.84,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.4% of the total average of 19.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,134852.5,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,69044480,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,134908.06,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,69072926,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Grid Size,,190,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Threads,thread,97280,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.48,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978746,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
288,==PROF== Connected to process 978910 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 292 x 292 x 292 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 10x19 GPU FDTD loopiter = 1dimx = 292gpu_time = 1761.461020fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 978910==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10273646771.15,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2241225057.97,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,860503,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,43.98,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,19.48,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,383872,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,59.6,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.5,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,634923.3,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,43.98,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.83,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.61,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.7,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.83,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.33,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,192106702234.08,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.39,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,43.98,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.66,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.67,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,43.98,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 82.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.7,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.3,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.43,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.43 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.3,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.31,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.77,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.9% of the total average of 19.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,131355.31,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,67253920,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,131410.79,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,67282326,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Block Size,,512,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Grid Size,,190,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Threads,thread,97280,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.48,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(10, 19, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,978910,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
280,==PROF== Connected to process 979083 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 284 x 284 x 284 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 9x18 GPU FDTD loopiter = 1dimx = 284gpu_time = 1711.779833fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979083==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10165575841.08,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2217603801.32,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,824203,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,38.09,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,17.9,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,371584,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,59.44,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,18.61,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,528046.29,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,38.09,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.83,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.53,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.66,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.83,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.27,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,174659662418.19,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,19.4,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,38.09,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.3,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.33,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,38.09,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.1 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.74,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.26,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.31,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.32,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.95,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.4% of the total average of 19.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,109015.88,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,55816128,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,109069.94,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,55843808,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Grid Size,,162,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Threads,thread,82944,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.27,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979083,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
272,==PROF== Connected to process 979233 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 276 x 276 x 276 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 9x18 GPU FDTD loopiter = 1dimx = 276gpu_time = 1776.741982fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979233==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10199916052.05,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2225170791.84,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,791806,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,38.54,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,16.33,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,355776,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.01,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,18.44,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,508433.1,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,38.54,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.83,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.54,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.87,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.83,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.48,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,159889908256.88,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,19.63,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,38.54,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.49,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.28,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,38.54,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.84,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.16,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.13,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.14,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.9% of the total average of 19.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,106034.06,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,54289440,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,106088.74,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,54317433,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Grid Size,,162,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Threads,thread,82944,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.27,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 18, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979233,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
264,==PROF== Connected to process 979379 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 268 x 268 x 268 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 9x17 GPU FDTD loopiter = 1dimx = 268gpu_time = 1829.453230fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979379==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10189006329.31,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2222290042.18,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,760459,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,36.82,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,14.75,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,342112,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.42,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,17.55,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,463273.52,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,36.82,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.84,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.51,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.02,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.84,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.62,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,144249555701.06,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,18.75,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,36.82,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.5,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.13,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,36.82,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 83.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.05,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.95,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.03,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.04,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.88,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.1% of the total average of 19.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,97327.12,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,49831488,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,97380.81,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,49858977,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Grid Size,,153,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Threads,thread,78336,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.2,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979379,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
256,==PROF== Connected to process 979532 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 260 x 260 x 260 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 9x17 GPU FDTD loopiter = 1dimx = 260gpu_time = 1706.510067fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979532==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10233176838.81,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2232418262.73,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,730489,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,37.19,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,13.17,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,327168,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,61.01,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,17.08,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,445225.6,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,37.19,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.85,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.52,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.24,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.85,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.83,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,129413536776.21,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,18.94,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,37.19,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.04,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.73,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,37.19,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 81.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.13,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.87,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.88,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.89,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.8,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.5% of the total average of 18.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,94510.97,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,48389616,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,94566.44,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,48418016,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Grid Size,,153,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Threads,thread,78336,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Launch Statistics,Waves Per SM,,1.2,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(9, 17, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979532,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
248,==PROF== Connected to process 979671 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 252 x 252 x 252 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 8x16 GPU FDTD loopiter = 1dimx = 252gpu_time = 1457.454920fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979671==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10173410404.62,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2218514802.6,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,380869,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,57.74,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,21.7,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,171616,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.23,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,27.98,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,364995.96,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,57.74,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.84,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.8,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,20.89,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.84,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.56,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,211916464665.3,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,29.41,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,57.74,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.25,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.26,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,57.74,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 87.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,20.88,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,No Eligible,%,79.12,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,19.11,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,19.13,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.9,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.3 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.8% of the total average of 19.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,76200,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,39014400,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,76253.57,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,39041830,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Grid Size,,128,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Threads,thread,65536,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Waves Per SM,,1,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979671,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
240,==PROF== Connected to process 979803 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 244 x 244 x 244 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 8x16 GPU FDTD loopiter = 1dimx = 244gpu_time = 1648.695946fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979803==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10158923584.67,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2215765747.25,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,372534,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,57.18,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,20.04,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,168064,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.47,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,27.1,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,352109.29,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,57.18,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.84,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.79,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.03,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.84,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.64,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,195402132520.94,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,29.12,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,57.18,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.35,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.25,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,57.18,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 85.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.05,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.95,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.8 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.98,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.99,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.81,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.3% of the total average of 19.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,73992,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,37883904,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,74044.82,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,37910946,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Grid Size,,128,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Threads,thread,65536,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Launch Statistics,Waves Per SM,,1,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.""0",979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 16, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979803,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
232,==PROF== Connected to process 979928 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 236 x 236 x 236 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 8x15 GPU FDTD loopiter = 1dimx = 236gpu_time = 1557.240963fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 979928==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10204880665.06,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2226000260.55,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,354271,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,54.53,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,18.64,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,159104,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.87,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,26.03,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,317313.04,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,54.53,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.85,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.76,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.18,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.85,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.78,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,182572003218.02,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,27.77,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,54.53,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.24,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.1,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,54.53,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 83.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.23,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.77,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.03,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.45,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.03 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.98,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.99,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.82,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.2% of the total average of 19.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,67158.75,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,34385280,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,67210.1,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,34411570,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Block Size,,512,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Grid Size,,120,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Threads,thread,61440,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.94,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 120 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,979928,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
224,==PROF== Connected to process 980059 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 228 x 228 x 228 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 8x15 GPU FDTD loopiter = 1dimx = 228gpu_time = 1550.703049fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980059==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10319176852.43,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2250822786.6,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,347778,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,53.69,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,17.17,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,154464,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,60.92,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,24.41,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,306411.81,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,53.69,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.9 full waves across all SMs. Look at Launch Statistics for more details.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.85,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.75,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.21,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.85,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,21.8,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,170104827014.71,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,27.34,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,53.69,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.55,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.5,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,53.69,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.5 sectors per request, or 2.5*32 = 81.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.18,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.82,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.98,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.44,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.98 active warps per scheduler, but only an average of 0.44 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.79,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.8,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.73,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.1% of the total average of 18.8 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,64950,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,33254400,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,65001.28,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,33280657,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Grid Size,,120,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Threads,thread,61440,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.94,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 120 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(8, 15, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980059,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
216,==PROF== Connected to process 980184 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 220 x 220 x 220 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 7x14 GPU FDTD loopiter = 1dimx = 220gpu_time = 1505.007982fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980184==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10135517712.15,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2211178325.07,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,328300,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.83,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,16.4,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,148448,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,61.66,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,22.07,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,238643.25,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.83,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.8 full waves across all SMs. Look at Launch Statistics for more details.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.86,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.62,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.49,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.86,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.07,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,159599913774.52,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.83,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.83,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.67,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.17,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.83,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.9 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.46,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.21,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.54,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.45,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.61,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.62,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.96,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 6.0 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.2% of the total average of 18.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,51238.69,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,26234208,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,51280.54,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,26255636,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Grid Size,,98,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Threads,thread,50176,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.77,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 98 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980184,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
208,==PROF== Connected to process 980301 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 212 x 212 x 212 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 7x14 GPU FDTD loopiter = 1dimx = 212gpu_time = 1386.604071fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980301==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10252935057.22,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237136115.39,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,321194,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,44.17,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,15.06,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,143552,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,61.63,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.9,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,230173.04,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,44.17,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.8 full waves across all SMs. Look at Launch Statistics for more details.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.86,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.62,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.5,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.86,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.05,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,148262148907.71,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,22.49,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,44.17,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,9.97,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.92,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,44.17,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.6 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.52,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.48,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.45,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.59,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.61,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.9% of the total average of 18.6 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,49434.88,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,25310656,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,49476.61,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,25332024,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Grid Size,,98,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Threads,thread,50176,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.77,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 98 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 14, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980301,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
200,==PROF== Connected to process 980421 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 204 x 204 x 204 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 7x13 GPU FDTD loopiter = 1dimx = 204gpu_time = 1501.557827fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980421==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10205384798.67,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2226534239.82,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,299067,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,42.41,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,14.47,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,134304,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.35,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,20.31,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,203385.05,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,42.41,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full waves across all SMs. Look at Launch Statistics for more details.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.87,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.59,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.77,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.87,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.31,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,141801286633.31,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.6,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,42.41,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.18,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.97,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,42.41,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 82.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.81,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.19,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.03,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.03 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.46,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.48,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.87,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.9% of the total average of 18.5 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,44228.84,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,22645168,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,44266.84,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,22664621,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Grid Size,,91,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Threads,thread,46592,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.71,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 91 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980421,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
192,==PROF== Connected to process 980531 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 196 x 196 x 196 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 7x13 GPU FDTD loopiter = 1dimx = 196gpu_time = 1475.557089fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980531==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10112855187.96,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2206474610.88,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,292412,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,41.69,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,13.19,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,132512,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.2,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,19.06,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,195960.79,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,41.69,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.7 full waves across all SMs. Look at Launch Statistics for more details.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.87,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.58,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.74,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.87,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.26,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,128044433711.66,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,21.23,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,41.69,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.8,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.67,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,41.69,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.5 sectors per request, or 2.5*32 = 80.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.74,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.26,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.42,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.44,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.77,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.6% of the total average of 18.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,42553.88,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,21787584,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,42592.94,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,21807585,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Grid Size,,91,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Threads,thread,46592,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.71,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 91 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(7, 13, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980531,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
184,==PROF== Connected to process 980645 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 188 x 188 x 188 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 6x12 GPU FDTD loopiter = 1dimx = 188gpu_time = 1477.179050fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980645==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10273280219.31,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2241597012.92,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,279128,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,33.16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,12.25,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,124512,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.47,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,15.88,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,148155.22,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,33.16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.87,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.46,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.85,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.87,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.35,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,120858391159.09,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,16.89,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,33.16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.87,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.09,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,33.16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.84,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.32,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.34,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,27.03,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.9% of the total average of 18.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,32343.75,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,16560000,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,32374.63,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,16575811,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Grid Size,,72,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Threads,thread,36864,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.56,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 72 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980645,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
176,==PROF== Connected to process 980760 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 180 x 180 x 180 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 6x12 GPU FDTD loopiter = 1dimx = 180gpu_time = 1465.625048fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980760==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10271631395.66,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2241362357.84,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,267769,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,33.01,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,11.27,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,119456,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.45,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,15.48,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,141514.32,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,33.01,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full waves across all SMs. Look at Launch Statistics for more details.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.87,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.46,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.74,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.87,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.35,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,111157781944.82,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,16.81,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,33.01,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.38,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.54,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,33.01,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 84.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.72,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.28,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.45,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.45 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.38,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.4,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.76,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.8% of the total average of 18.4 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,30730.5,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,15734016,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,30761.35,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,15749813,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Grid Size,,72,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Threads,thread,36864,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.56,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 72 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 12, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980760,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
168,==PROF== Connected to process 980865 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 172 x 172 x 172 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 6x11 GPU FDTD loopiter = 1dimx = 172gpu_time = 1484.987974fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980865==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10224740321.06,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2230739763.07,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,251997,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,30.74,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,10.53,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,112960,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.68,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,14.73,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,123567.63,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,30.74,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.43,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.9,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.88,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.43,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,103315580736.54,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,15.65,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,30.74,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.17,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.69,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,30.74,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.5 sectors per request, or 2.5*32 = 81.3 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.02,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.98,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.23,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.25,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.79,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.9 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.2% of the total average of 18.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,27031.12,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,13839936,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,27059.1,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,13854259,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Grid Size,,66,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Threads,thread,33792,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.52,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 66 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.33,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980865,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
160,==PROF== Connected to process 980970 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 164 x 164 x 164 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 6x11 GPU FDTD loopiter = 1dimx = 164gpu_time = 1568.138123fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 980970==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10241477551.82,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2234725403.91,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,242652,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,30.45,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,9.53,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,108576,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.65,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,13.87,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,117943.26,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,30.45,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.5 full waves across all SMs. Look at Launch Statistics for more details.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.43,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.91,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.88,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.42,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,93715296198.05,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,15.51,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,30.45,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.66,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.6,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,30.45,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.5 sectors per request, or 2.5*32 = 78.7 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.92,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.08,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.6 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.29,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.31,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.66,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.6% of the total average of 18.3 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,25816.31,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,13217952,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,25844.68,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,13232474,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Block Size,,512,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Grid Size,,66,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Threads,thread,33792,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.52,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 66 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(6, 11, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,980970,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
152,==PROF== Connected to process 981074 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 156 x 156 x 156 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 5x10 GPU FDTD loopiter = 1dimx = 156gpu_time = 1402.311087fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981074==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10253568429.89,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237365289.39,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,227400,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,23.43,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,8.81,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,101632,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,63.11,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,11.36,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,84412.01,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,23.43,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full waves across all SMs. Look at Launch Statistics for more details.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.33,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.1,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.88,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.59,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,86715365239.29,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,11.94,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,23.43,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.43,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.7,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,23.43,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.4 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.13,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.87,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.13,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.15,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.98,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.2% of the total average of 18.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,18637.5,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,9542400,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,18658.99,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,9553403,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Grid Size,,50,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Threads,thread,25600,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.39,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 50 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,981074,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
144,==PROF== Connected to process 981178 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 148 x 148 x 148 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 5x10 GPU FDTD loopiter = 1dimx = 148gpu_time = 1435.765982fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981178==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10321929824.56,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2252587657,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,219145,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,23.08,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,7.87,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,97280,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.66,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,10.66,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,80709.38,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,23.08,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full waves across all SMs. Look at Launch Statistics for more details.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.32,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,21.98,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.88,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.43,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,77959210526.32,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,11.75,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,23.08,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.26,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.88,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,23.08,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 83.2 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,21.98,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,No Eligible,%,78.02,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.46,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.46 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.2,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.22,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.83,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.7% of the total average of 18.2 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,17717.19,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,9071200,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,17738.67,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,9082197,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Grid Size,,50,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Threads,thread,25600,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.39,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 50 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 10, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,981178,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
136,==PROF== Connected to process 981278 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 140 x 140 x 140 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 5x9 GPU FDTD loopiter = 1dimx = 140gpu_time = 1519.191027fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981278==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10233681765.39,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2233098075.7,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,205100,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,21.01,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,7.18,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,91840,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.99,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,9.81,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,68401.7,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,21.01,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full waves across all SMs. Look at Launch Statistics for more details.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.29,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.13,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.54,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,70508710801.39,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,10.7,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,21.01,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.86,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.79,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,21.01,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.5 sectors per request, or 2.5*32 = 80.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.14,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.86,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.00 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.04,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.07,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.85,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.2% of the total average of 18.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,15117.19,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,7740000,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,15136.4,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,7749837,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Grid Size,,45,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Threads,thread,23040,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.35,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 45 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,16,0,981278,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
128,==PROF== Connected to process 981383 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 132 x 132 x 132 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 5x9 GPU FDTD loopiter = 1dimx = 132gpu_time = 1366.196871fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981383==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10277329420.4,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2242639273.33,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,195640,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,20.78,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,6.37,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,87232,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.88,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,9.18,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,64659.93,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,20.78,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.4 full waves across all SMs. Look at Launch Statistics for more details.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.88,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.29,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.13,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.5,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,62820249449.74,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,10.58,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,20.78,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.81,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,74.05,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,20.78,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.4 sectors per request, or 2.4*32 = 76.8 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.15,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.85,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.09,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.11,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.71,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.4% of the total average of 18.1 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,14288.91,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,7315920,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,14308.16,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,7325777,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Grid Size,,45,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Threads,thread,23040,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.35,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 45 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(5, 9, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,15.99,0,981383,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
120,==PROF== Connected to process 981476 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 124 x 124 x 124 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 4x8 GPU FDTD loopiter = 1dimx = 124gpu_time = 1368.991852fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981476==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10253980786.95,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2237610137.1,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,181379,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,14.99,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,5.76,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,81056,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,63.29,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,7.47,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,42953,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,14.99,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.89,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.21,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.32,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.65,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,56661666008.69,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,7.64,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,14.99,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,8.71,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.53,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,14.99,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.7 sectors per request, or 2.7*32 = 86.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.9 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.26,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.74,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.01,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.01 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18.02,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.05,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,27.09,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.8% of the total average of 18.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,9572,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,4900864,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,9585.73,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,4907893,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Grid Size,,32,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Threads,thread,16384,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.25,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,15.99,0,981476,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
112,==PROF== Connected to process 981571 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 116 x 116 x 116 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 4x8 GPU FDTD loopiter = 1dimx = 116gpu_time = 1242.819071fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981571==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10188062595.21,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2223159487.48,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,171246,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,14.87,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,5.05,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,77024,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.82,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,6.95,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,40519.74,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,14.87,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.89,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.21,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.2,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.48,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,49414208558.37,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,7.57,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,14.87,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,8.56,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,72.9,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,14.87,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.6 sectors per request, or 2.6*32 = 82.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.22,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.78,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,17.98,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.92,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.6 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.4% of the total average of 18.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,8983,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,4599296,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,8996.5,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,4606210,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Grid Size,,32,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Threads,thread,16384,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.25,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 32 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 8, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,15.99,0,981571,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
104,==PROF== Connected to process 981661 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 108 x 108 x 108 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 4x7 GPU FDTD loopiter = 1dimx = 108gpu_time = 1437.032938fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981661==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10140382862.35,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2213077245.79,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,155384,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,13.29,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,4.56,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,70208,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,63.39,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,6.16,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,32573.6,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,13.29,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.89,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.19,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.24,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.69,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,44379216043.76,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,6.77,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,13.29,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,11.54,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,71.93,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,13.29,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.4 sectors per request, or 2.4*32 = 78.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.6 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.25,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.75,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,3.99,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 3.99 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,17.95,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,17.97,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.71,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.8 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 32.3% of the total average of 17.9 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,7232.75,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3703168,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,7244.79,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3709333,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Grid Size,,28,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Threads,thread,14336,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.22,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 28 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,15.99,0,981661,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
96,==PROF== Connected to process 981763 (/home/hemeng/Documents/hemeng/test_0309/FDTD3d_fp16_ncu)/home/hemeng/Documents/hemeng/test_0309/./FDTD3d_fp16_ncu Starting...Set-up," based upon target device GMEM size... getTargetDeviceGlobalMemSize cudaGetDeviceCountGPU Device 0: ""Ada"" with compute capability 8.9 cudaGetDevicePropertiesmemsize = 12696846336defaultDim = 1164 generateRandomDataFDTD on 100 x 100 x 100 volume with symmetric filter radius 4 for 5 timesteps...fdtdGPU...GPU Device 0: ""Ada"" with compute capability 8.9DTYPE size is 2 set block size to 32x16 set grid size to 4x7 GPU FDTD loopiter = 1dimx = 100gpu_time = 1240.183115fdtdGPU completeCompareData (tolerance 0.000100)...==PROF== Disconnected from process 981763==ERROR== The application returned an error code (1).==PROF== Report: /home/hemeng/Documents/hemeng/test_0309/test.ncu-rep""ID""",Process ID,Process Name,Host Name,Kernel Name,Context,Stream,Block Size,Grid Size,Device,CC,Section Name,Metric Name,Metric Unit,Metric Value,Rule Name,Rule Type,"Rule Description""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Frequency,cycle/second,10116811874.8,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,SM Frequency,cycle/second,2208199034.15,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Elapsed Cycles,cycle,145995,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Memory Throughput,%,13.11,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,DRAM Throughput,%,3.91,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Duration,nsecond,66112,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,L1/TEX Cache Throughput,%,62.98,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,L2 Cache Throughput,%,5.67,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,SM Active Cycles,cycle,30388.52,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,GPU Speed Of Light Throughput,Compute (SM) Throughput,%,13.11,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SpeedOfLight,,,,SOLBottleneck,WRN,"This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SpeedOfLight_RooflineChart,,,,SOLFPRoofline,INF,"The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The kernel achieved 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Active,inst/cycle,0.89,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Executed Ipc Elapsed,inst/cycle,0.18,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Issue Slots Busy,%,22.25,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,Issued Ipc Active,inst/cycle,0.89,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Compute Workload Analysis,SM Busy,%,22.54,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,ComputeWorkloadAnalysis,,,,HighPipeUtilization,WRN,"All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Memory Throughput,byte/second,38017424975.8,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Mem Busy,%,6.67,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Max Bandwidth,%,13.11,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L1/TEX Hit Rate,%,10.88,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Compression Success Rate,%,0,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Compression Ratio,,0,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,L2 Hit Rate,%,73.68,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Memory Workload Analysis,Mem Pipes Busy,%,13.11,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 2.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 2.3 sectors per request, or 2.3*32 = 74.0 bytes of cache data transfers per request. The optimal thread address pattern for 2.0 byte accesses would result in 2.0*32 = 64.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.5 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,MemoryWorkloadAnalysis_Tables,,,,MemoryCacheAccessPattern,WRN,"The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.7 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,One or More Eligible,%,22.31,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Issued Warp Per Scheduler,,0.22,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,No Eligible,%,77.69,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Active Warps Per Scheduler,warp,4.02,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Scheduler Statistics,Eligible Warps Per Scheduler,warp,0.47,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN,"Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 4.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 12 warps per scheduler, this kernel allocates an average of 4.02 active warps per scheduler, but only an average of 0.47 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,SchedulerStats,,,,IssueSlotUtilization,WRN," The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 12. Use the Occupancy section to identify what limits this kernel's theoretical occupancy.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Issued Instruction,cycle,18,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Warp Cycles Per Executed Instruction,cycle,18.03,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Avg. Active Threads Per Warp,,32,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Warp State Statistics,Avg. Not Predicated Off Threads Per Warp,,26.54,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,WarpStateStats,,,,CPIStall,WRN,"On average, each warp of this kernel spends 5.7 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture, rtcore) operation. This represents about 31.6% of the total average of 18.0 cycles between issuing two instructions. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality or by changing the cache configuration, and consider moving frequently used data to registers and to shared memory.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,WarpStateStats,,,,CPIStall,INF,"Check the Source Counters section for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#sampling) provides more details on each stall reason.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Avg. Executed Instructions Per Scheduler,inst,6749.75,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Executed Instructions,inst,3455872,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Avg. Issued Instructions Per Scheduler,inst,6761.79,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Instruction Statistics,Issued Instructions,inst,3462034,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Block Size,,512,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Function Cache Configuration,,CachePreferNone,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Grid Size,,28,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Registers Per Thread,register/thread,80,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Shared Memory Configuration Size,byte,8192,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Driver Shared Memory Per Block,byte/block,1024,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Dynamic Shared Memory Per Block,byte/block,0,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Static Shared Memory Per Block,byte/block,1920,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Threads,thread,14336,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Launch Statistics,Waves Per SM,,0.22,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,LaunchStats,,,,LaunchConfiguration,WRN,"The grid for this launch is configured to execute only 28 blocks, which is less than the GPU's 128 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.""0",981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit SM,block,24,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Registers,block,1,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Shared Mem,block,2,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Block Limit Warps,block,3,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Theoretical Active Warps per SM,warp,16,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Theoretical Occupancy,%,33.33,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Achieved Occupancy,%,33.32,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)","(4, 7, 1)",0,8.9,Occupancy,Achieved Active Warps Per SM,warp,15.99,0,981763,FDTD3d_fp16_ncu,127.0.0.1,"FiniteDifferencesKernel(__half *, const __half *, int, int, int)",1,7,"(32, 16, 1)"
